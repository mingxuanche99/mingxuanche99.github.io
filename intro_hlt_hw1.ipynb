{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro-hlt-hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingxuanche99/mingxuanche99.github.io/blob/main/intro_hlt_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLm_JxCxrkIE"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "In the first assignment, you will implement some of the algorithms you have learnt in the first two weeks of lectures: n-gram language models, and syntactic parsing using the CYK algorithm. \n",
        "\n",
        "# Setup\n",
        "\n",
        "For this and other assignments, we will be using Google Colab, for both code as well as descriptive questions. Your task is to finish all the questions in the Colab notebook and then upload a PDF version of the notebook, and a viewable link on Gradescope.\n",
        "\n",
        "### Google colaboratory\n",
        "\n",
        "Before getting started, get familiar with google colaboratory:\n",
        "https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "This is a neat python environment that works in the cloud and does not require you to\n",
        "set up anything on your personal machine\n",
        "(it also has some built-in IDE features that make writing code easier).\n",
        "Moreover, it allows you to copy any existing collaboratory file, alter it and share\n",
        "with other people.\n",
        "\n",
        "### Submission\n",
        "\n",
        "Before you start working on this homework do the following steps:\n",
        "\n",
        "1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n",
        "2. Follow all the steps in this collaboratory file and write / change / uncomment code as necessary.\n",
        "3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n",
        "4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected. Copy the link and paste it in the box below.\n",
        "5. After completing the notebook, press __File > Download .ipynb__ to download a local copy on your computer, and then use `jupyter nbconvert --to pdf intro-hlt-hw1.ipynb` to convert the notebook into PDF format for uploading on Gradescope.\n",
        "\n",
        "__Paste your notebook link in the box below.__ _(0 points)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A7c68XN-XDn"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Paste your Colab notebook link here\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXKLhH1N0IU"
      },
      "source": [
        "## Part 1: N-gram Language Models\n",
        "\n",
        "For the first part of this assignment, you will implement a trigram language model and train it on a small corpus. You will then implement a scoring function to compute the perplexity of the model on a held-out test set. Finally, you will implement some methods to deal with sparsity (zero count) issues in your model.\n",
        "\n",
        "To ease you into the implementation, we will provide some boilerplate code that you would need to fill in depending upon the functionalities the code is supposed to perform. For the first few sections below, we will use the complete text from Leo Tolstoy's \"War and Peace,\" which is freely available from [Project Gutenberg](https://www.gutenberg.org/). Run the following code block to download the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeaIH26iQiRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62567212-ffe0-415b-d140-190763aa4023"
      },
      "source": [
        "# Download the War and Peace text.\n",
        "! wget https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-20 01:52:30--  https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3258246 (3.1M) [text/plain]\n",
            "Saving to: ‘warpeace_input.txt’\n",
            "\n",
            "warpeace_input.txt  100%[===================>]   3.11M  6.18MB/s    in 0.5s    \n",
            "\n",
            "2021-09-20 01:52:30 (6.18 MB/s) - ‘warpeace_input.txt’ saved [3258246/3258246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obJJmlMlSs2d"
      },
      "source": [
        "The complete text downloaded above contains punctuations which are not important for our purposes. So we will perform a basic text preprocessing using the [NLTK toolkit](https://www.nltk.org/). We will store the processed text into a list of strings, where each string will contain words without any punctuations. We will also convert all words to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoKZKaRGVYa1"
      },
      "source": [
        "# Loading the text\n",
        "with open('warpeace_input.txt', 'r') as file:\n",
        "    corpus_raw = file.read().replace('\\n', ' ')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r_Q1XbTWCst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07370e7-f2e0-43ab-a516-828141fa05b1"
      },
      "source": [
        "# Text pre-processing to remove punctuation marks\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(corpus_raw)\n",
        "\n",
        "corpus = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for sentence in sentences:\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  corpus.append(\" \".join([token.lower() for token in tokens]))\n",
        "\n",
        "print (\"Corpus has {} sentences\".format(len(corpus)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Corpus has 32040 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykO86GMkPQ2E"
      },
      "source": [
        "### Warm-up: n-gram counts from a corpus\n",
        "\n",
        "Let's start with implementing a simple function for obtaining n-grams and their counts from a string. Complete the following function. _(5 points)_\n",
        "\n",
        "__Note 1:__ Use the special token `~` for both beginning of sentence (BOS) and end of sentence (EOS) tokens. For example, the sentence \"Mary has a little lamb\" has the bigrams \"_~ Mary_\", \"_Mary has_\", \"_has a_\", \"_a little_\", \"_little lamb_\", and \"_lamb ~_\".\n",
        "\n",
        "__Note 2:__ You don't need to do any further text processing beyond what has already been done before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7lexIbPEiY"
      },
      "source": [
        "def generate_ngrams(text, n):\n",
        "  \"\"\"Generate all n-grams (i.e. n-1 context words) for the given text.\n",
        "\n",
        "    Parameters:\n",
        "      text (list(str)): input text (list of strings)\n",
        "      n (int): n-gram parameter (must be greater than or equal to 2)\n",
        "\n",
        "    Returns:\n",
        "      ngrams (dict(ngram: count)): output n-grams dictionary, indexed by \n",
        "        n-gram tuple, e.g. ('Mary','has') and value as count of the n-gram\n",
        "        in the text. \n",
        "  \"\"\"\n",
        "  assert (isinstance(n, int) and n > 0)\n",
        "\n",
        "  ngrams = {}\n",
        "  for tem in range(len(text)):\n",
        "    sentance=text[tem].split()\n",
        "    for i in range(n-1):\n",
        "      sentance.append('~')\n",
        "      sentance.insert(0,'~')\n",
        "    counts = 1\n",
        "    if len(sentance) <= n:\n",
        "      div = tuple(sentance)\n",
        "      if ngrams.get(div) is None:\n",
        "        ngrams[div] = 1\n",
        "      else:\n",
        "        ngrams[div]=ngrams.get(div)+1\n",
        "    else:\n",
        "      for tem in range(len(sentance)):\n",
        "        lists = []\n",
        "        if tem + n - 1 < len(sentance):\n",
        "          for nus in range(n):\n",
        "            ten = tem + nus\n",
        "            lists.append(sentance[ten])\n",
        "          div = tuple(lists)\n",
        "          if ngrams.get(div) is None:\n",
        "            ngrams[div] = 1\n",
        "          else:\n",
        "            ngrams[div] = ngrams.get(div) + 1\n",
        "  # TODO: Your code here.\n",
        "\n",
        "  return ngrams"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5XK61TRfuyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b69723d-7f1d-4729-e357-44965925c05c"
      },
      "source": [
        "# Test your implementation on some sentences from the corpus we downloaded above\n",
        "# and verify if it works correctly.\n",
        "\n",
        "text = corpus[:10]\n",
        "print (text)\n",
        "ngrams = generate_ngrams(text,3)\n",
        "print (ngrams)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['well prince so genoa and lucca are now just family estates of the buonapartes', 'but i warn you if you don t tell me that this means war if you still try to defend the infamies and horrors perpetrated by that antichrist i really believe he is antichrist i will have nothing more to do with you and you are no longer my friend no longer my faithful slave as you call yourself', 'but how do you do', 'i see i have frightened you sit down and tell me all the news', 'it was in july 1805 and the speaker was the well known anna pavlovna scherer maid of honor and favorite of the empress marya fedorovna', 'with these words she greeted prince vasili kuragin a man of high rank and importance who was the first to arrive at her reception', 'anna pavlovna had had a cough for some days', 'she was as she said suffering from la grippe grippe being then a new word in st petersburg used only by the elite', 'all her invitations without exception written in french and delivered by a scarlet liveried footman that morning ran as follows if you have nothing better to do count or prince and if the prospect of spending an evening with a poor invalid is not too terrible i shall be very charmed to see you tonight between 7 and 10 annette scherer', 'heavens']\n",
            "{('~', '~', 'well'): 1, ('~', 'well', 'prince'): 1, ('well', 'prince', 'so'): 1, ('prince', 'so', 'genoa'): 1, ('so', 'genoa', 'and'): 1, ('genoa', 'and', 'lucca'): 1, ('and', 'lucca', 'are'): 1, ('lucca', 'are', 'now'): 1, ('are', 'now', 'just'): 1, ('now', 'just', 'family'): 1, ('just', 'family', 'estates'): 1, ('family', 'estates', 'of'): 1, ('estates', 'of', 'the'): 1, ('of', 'the', 'buonapartes'): 1, ('the', 'buonapartes', '~'): 1, ('buonapartes', '~', '~'): 1, ('~', '~', 'but'): 2, ('~', 'but', 'i'): 1, ('but', 'i', 'warn'): 1, ('i', 'warn', 'you'): 1, ('warn', 'you', 'if'): 1, ('you', 'if', 'you'): 1, ('if', 'you', 'don'): 1, ('you', 'don', 't'): 1, ('don', 't', 'tell'): 1, ('t', 'tell', 'me'): 1, ('tell', 'me', 'that'): 1, ('me', 'that', 'this'): 1, ('that', 'this', 'means'): 1, ('this', 'means', 'war'): 1, ('means', 'war', 'if'): 1, ('war', 'if', 'you'): 1, ('if', 'you', 'still'): 1, ('you', 'still', 'try'): 1, ('still', 'try', 'to'): 1, ('try', 'to', 'defend'): 1, ('to', 'defend', 'the'): 1, ('defend', 'the', 'infamies'): 1, ('the', 'infamies', 'and'): 1, ('infamies', 'and', 'horrors'): 1, ('and', 'horrors', 'perpetrated'): 1, ('horrors', 'perpetrated', 'by'): 1, ('perpetrated', 'by', 'that'): 1, ('by', 'that', 'antichrist'): 1, ('that', 'antichrist', 'i'): 1, ('antichrist', 'i', 'really'): 1, ('i', 'really', 'believe'): 1, ('really', 'believe', 'he'): 1, ('believe', 'he', 'is'): 1, ('he', 'is', 'antichrist'): 1, ('is', 'antichrist', 'i'): 1, ('antichrist', 'i', 'will'): 1, ('i', 'will', 'have'): 1, ('will', 'have', 'nothing'): 1, ('have', 'nothing', 'more'): 1, ('nothing', 'more', 'to'): 1, ('more', 'to', 'do'): 1, ('to', 'do', 'with'): 1, ('do', 'with', 'you'): 1, ('with', 'you', 'and'): 1, ('you', 'and', 'you'): 1, ('and', 'you', 'are'): 1, ('you', 'are', 'no'): 1, ('are', 'no', 'longer'): 1, ('no', 'longer', 'my'): 2, ('longer', 'my', 'friend'): 1, ('my', 'friend', 'no'): 1, ('friend', 'no', 'longer'): 1, ('longer', 'my', 'faithful'): 1, ('my', 'faithful', 'slave'): 1, ('faithful', 'slave', 'as'): 1, ('slave', 'as', 'you'): 1, ('as', 'you', 'call'): 1, ('you', 'call', 'yourself'): 1, ('call', 'yourself', '~'): 1, ('yourself', '~', '~'): 1, ('~', 'but', 'how'): 1, ('but', 'how', 'do'): 1, ('how', 'do', 'you'): 1, ('do', 'you', 'do'): 1, ('you', 'do', '~'): 1, ('do', '~', '~'): 1, ('~', '~', 'i'): 1, ('~', 'i', 'see'): 1, ('i', 'see', 'i'): 1, ('see', 'i', 'have'): 1, ('i', 'have', 'frightened'): 1, ('have', 'frightened', 'you'): 1, ('frightened', 'you', 'sit'): 1, ('you', 'sit', 'down'): 1, ('sit', 'down', 'and'): 1, ('down', 'and', 'tell'): 1, ('and', 'tell', 'me'): 1, ('tell', 'me', 'all'): 1, ('me', 'all', 'the'): 1, ('all', 'the', 'news'): 1, ('the', 'news', '~'): 1, ('news', '~', '~'): 1, ('~', '~', 'it'): 1, ('~', 'it', 'was'): 1, ('it', 'was', 'in'): 1, ('was', 'in', 'july'): 1, ('in', 'july', '1805'): 1, ('july', '1805', 'and'): 1, ('1805', 'and', 'the'): 1, ('and', 'the', 'speaker'): 1, ('the', 'speaker', 'was'): 1, ('speaker', 'was', 'the'): 1, ('was', 'the', 'well'): 1, ('the', 'well', 'known'): 1, ('well', 'known', 'anna'): 1, ('known', 'anna', 'pavlovna'): 1, ('anna', 'pavlovna', 'scherer'): 1, ('pavlovna', 'scherer', 'maid'): 1, ('scherer', 'maid', 'of'): 1, ('maid', 'of', 'honor'): 1, ('of', 'honor', 'and'): 1, ('honor', 'and', 'favorite'): 1, ('and', 'favorite', 'of'): 1, ('favorite', 'of', 'the'): 1, ('of', 'the', 'empress'): 1, ('the', 'empress', 'marya'): 1, ('empress', 'marya', 'fedorovna'): 1, ('marya', 'fedorovna', '~'): 1, ('fedorovna', '~', '~'): 1, ('~', '~', 'with'): 1, ('~', 'with', 'these'): 1, ('with', 'these', 'words'): 1, ('these', 'words', 'she'): 1, ('words', 'she', 'greeted'): 1, ('she', 'greeted', 'prince'): 1, ('greeted', 'prince', 'vasili'): 1, ('prince', 'vasili', 'kuragin'): 1, ('vasili', 'kuragin', 'a'): 1, ('kuragin', 'a', 'man'): 1, ('a', 'man', 'of'): 1, ('man', 'of', 'high'): 1, ('of', 'high', 'rank'): 1, ('high', 'rank', 'and'): 1, ('rank', 'and', 'importance'): 1, ('and', 'importance', 'who'): 1, ('importance', 'who', 'was'): 1, ('who', 'was', 'the'): 1, ('was', 'the', 'first'): 1, ('the', 'first', 'to'): 1, ('first', 'to', 'arrive'): 1, ('to', 'arrive', 'at'): 1, ('arrive', 'at', 'her'): 1, ('at', 'her', 'reception'): 1, ('her', 'reception', '~'): 1, ('reception', '~', '~'): 1, ('~', '~', 'anna'): 1, ('~', 'anna', 'pavlovna'): 1, ('anna', 'pavlovna', 'had'): 1, ('pavlovna', 'had', 'had'): 1, ('had', 'had', 'a'): 1, ('had', 'a', 'cough'): 1, ('a', 'cough', 'for'): 1, ('cough', 'for', 'some'): 1, ('for', 'some', 'days'): 1, ('some', 'days', '~'): 1, ('days', '~', '~'): 1, ('~', '~', 'she'): 1, ('~', 'she', 'was'): 1, ('she', 'was', 'as'): 1, ('was', 'as', 'she'): 1, ('as', 'she', 'said'): 1, ('she', 'said', 'suffering'): 1, ('said', 'suffering', 'from'): 1, ('suffering', 'from', 'la'): 1, ('from', 'la', 'grippe'): 1, ('la', 'grippe', 'grippe'): 1, ('grippe', 'grippe', 'being'): 1, ('grippe', 'being', 'then'): 1, ('being', 'then', 'a'): 1, ('then', 'a', 'new'): 1, ('a', 'new', 'word'): 1, ('new', 'word', 'in'): 1, ('word', 'in', 'st'): 1, ('in', 'st', 'petersburg'): 1, ('st', 'petersburg', 'used'): 1, ('petersburg', 'used', 'only'): 1, ('used', 'only', 'by'): 1, ('only', 'by', 'the'): 1, ('by', 'the', 'elite'): 1, ('the', 'elite', '~'): 1, ('elite', '~', '~'): 1, ('~', '~', 'all'): 1, ('~', 'all', 'her'): 1, ('all', 'her', 'invitations'): 1, ('her', 'invitations', 'without'): 1, ('invitations', 'without', 'exception'): 1, ('without', 'exception', 'written'): 1, ('exception', 'written', 'in'): 1, ('written', 'in', 'french'): 1, ('in', 'french', 'and'): 1, ('french', 'and', 'delivered'): 1, ('and', 'delivered', 'by'): 1, ('delivered', 'by', 'a'): 1, ('by', 'a', 'scarlet'): 1, ('a', 'scarlet', 'liveried'): 1, ('scarlet', 'liveried', 'footman'): 1, ('liveried', 'footman', 'that'): 1, ('footman', 'that', 'morning'): 1, ('that', 'morning', 'ran'): 1, ('morning', 'ran', 'as'): 1, ('ran', 'as', 'follows'): 1, ('as', 'follows', 'if'): 1, ('follows', 'if', 'you'): 1, ('if', 'you', 'have'): 1, ('you', 'have', 'nothing'): 1, ('have', 'nothing', 'better'): 1, ('nothing', 'better', 'to'): 1, ('better', 'to', 'do'): 1, ('to', 'do', 'count'): 1, ('do', 'count', 'or'): 1, ('count', 'or', 'prince'): 1, ('or', 'prince', 'and'): 1, ('prince', 'and', 'if'): 1, ('and', 'if', 'the'): 1, ('if', 'the', 'prospect'): 1, ('the', 'prospect', 'of'): 1, ('prospect', 'of', 'spending'): 1, ('of', 'spending', 'an'): 1, ('spending', 'an', 'evening'): 1, ('an', 'evening', 'with'): 1, ('evening', 'with', 'a'): 1, ('with', 'a', 'poor'): 1, ('a', 'poor', 'invalid'): 1, ('poor', 'invalid', 'is'): 1, ('invalid', 'is', 'not'): 1, ('is', 'not', 'too'): 1, ('not', 'too', 'terrible'): 1, ('too', 'terrible', 'i'): 1, ('terrible', 'i', 'shall'): 1, ('i', 'shall', 'be'): 1, ('shall', 'be', 'very'): 1, ('be', 'very', 'charmed'): 1, ('very', 'charmed', 'to'): 1, ('charmed', 'to', 'see'): 1, ('to', 'see', 'you'): 1, ('see', 'you', 'tonight'): 1, ('you', 'tonight', 'between'): 1, ('tonight', 'between', '7'): 1, ('between', '7', 'and'): 1, ('7', 'and', '10'): 1, ('and', '10', 'annette'): 1, ('10', 'annette', 'scherer'): 1, ('annette', 'scherer', '~'): 1, ('scherer', '~', '~'): 1, ('~', '~', 'heavens'): 1, ('~', 'heavens', '~'): 1, ('heavens', '~', '~'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igo0_GcfRcX7"
      },
      "source": [
        "__Question:__ Plot a histogram of counts vs. number of unigrams with that count (you can choose a subset of the corpus, say 500 sentences). Repeat for bigrams and trigrams. What observation can you make from the plots? _(5 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-70MZJiHSmLE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "5fe29e1c-685b-49f7-e79f-7b413ec72f5e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, tight_layout=True)\n",
        "\n",
        "# Unigram counts\n",
        "# TODO: Your code here\n",
        "text = corpus[:500]\n",
        "ngrams = generate_ngrams(text,2)\n",
        "new_dict={}\n",
        "for value in ngrams.values():\n",
        "  if new_dict.get(value) is None:\n",
        "    new_dict[value]=1\n",
        "  else:\n",
        "    new_dict[value]=new_dict.get(value)+1\n",
        "X=list(new_dict.keys())\n",
        "Y=list(new_dict.values())\n",
        "plt.subplot(1,3,1)\n",
        "plt.bar(X,Y)\n",
        "\n",
        "\n",
        "# Bigram counts\n",
        "# TODO: Your code here\n",
        "ngrams = generate_ngrams(text,3)\n",
        "new_dict={}\n",
        "for value in ngrams.values():\n",
        "  if new_dict.get(value) is None:\n",
        "    new_dict[value]=1\n",
        "  else:\n",
        "    new_dict[value]=new_dict.get(value)+1\n",
        "X=list(new_dict.keys())\n",
        "Y=list(new_dict.values())\n",
        "plt.subplot(1,3,2)\n",
        "plt.bar(X,Y)\n",
        "\n",
        "# Trigram counts\n",
        "# TODO: Your code here\n",
        "ngrams = generate_ngrams(text,4)\n",
        "new_dict={}\n",
        "for value in ngrams.values():\n",
        "  if new_dict.get(value) is None:\n",
        "    new_dict[value]=1\n",
        "  else:\n",
        "    new_dict[value]=new_dict.get(value)+1\n",
        "X=list(new_dict.keys())\n",
        "Y=list(new_dict.values())\n",
        "plt.subplot(1,3,3)\n",
        "plt.bar(X,Y)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bFCvVZVX8Ws"
      },
      "source": [
        "__Answer:__ With the increase of n, the maximum value of count decreases, and most data are concentrated in the area with the small value of count, which indicates that with the increase of N, the repeatability of training samples in N-Gram model decreases and specificity increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtFid2zSLIaU"
      },
      "source": [
        "### Implementing an n-gram language model\n",
        "\n",
        "Next, you will implement a class for an trigram (n=3) language model. This will be a barebones trigram LM, i.e., no smoothing or OOV handling is required. Complete\n",
        "the functions in the following `NgramLM` class. _(20 points)_\n",
        "\n",
        "Note that the class itself is\n",
        "for a general n-gram LM, but we will instantiate it with n=3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEDwFtlFMNhP"
      },
      "source": [
        "import random\n",
        "\n",
        "class NgramLM(object):\n",
        "  \"\"\"A basic n-gram language model without any smoothing.\"\"\"\n",
        "\n",
        "  def __init__(self, n):\n",
        "      self.n = n\n",
        "      self.vocab = set()\n",
        "      self.ngrams = {} # count(ABC)\n",
        "      self.contexts = {} # {AB:[C1,C2,C2]}\n",
        "\n",
        "  def get_vocab(self):\n",
        "      \"\"\"Returns the vocabulary, i.e., list of all words\"\"\"\n",
        "      # TODO: Your code here\n",
        "      pass\n",
        "\n",
        "  def update(self, text):\n",
        "      \"\"\"Updates the model n-grams based on the given text input\"\"\"\n",
        "      # TODO: Your code here\n",
        "      pass\n",
        "\n",
        "  def word_prob(self, context, word):\n",
        "      \"\"\"Returns the probability of a word given a context. The context is a \n",
        "      string of words, with length n-1.\"\"\"\n",
        "      # TODO: Your code here\n",
        "      pass\n",
        "\n",
        "  def random_word(self, context):\n",
        "      \"\"\"Generate a random word based on the given context\"\"\"\n",
        "      # TODO: Your code here\n",
        "      pass\n",
        "\n",
        "  def random_text(self, length):\n",
        "      \"\"\"Generate random text of the specified word length\"\"\"\n",
        "      # TODO: Your code here\n",
        "      pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMYrpWN4cyWj"
      },
      "source": [
        "#### Training the trigram LM\n",
        "\n",
        "In the next code block, you will train the model you implemented above for n=3, on the War and Peace corpus. You will also answer some basic questions about the corpus and training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8krtKhK7XLbM",
        "cellView": "code"
      },
      "source": [
        "ngramlm = NgramLM(3)\n",
        "for sentence in corpus:\n",
        "  ngramlm.update(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlxn8YSdasgO"
      },
      "source": [
        "__Question:__ What is the size of the training data (number of tokens)? _(1 point)_\n",
        "\n",
        "__Answer:__\n",
        "\n",
        "__Question:__ What is the size of the vocabulary? _(1 point)_\n",
        "\n",
        "__Answer:__ \n",
        "\n",
        "__Question:__ What is the time taken in seconds for training? Use `%%timeit` to time the above code block. _(1 point)_\n",
        "\n",
        "__Answer:__ \n",
        "\n",
        "__Question:__ How would the training time scale if you have a corpus containing 1 billion tokens? Is this training time reasonable? If not, can you think of ways to improve it? _(2 points)_\n",
        "\n",
        "__Answer:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxTPrtaKbzf5"
      },
      "source": [
        "#### Predicting/generating text using the trained LM\n",
        "\n",
        "One of the applications of an LM is to automatically predict the next word given a context (such as in Smart Keyboards), or to generate a piece of text of a given length. Use the `random_word` and `random_text` functions you implemented earlier to answer the questions below. For full credit, you need to show how you arrived at the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgQQ-KgTeGFm"
      },
      "source": [
        "__Question:__ Consider the context \"by her\". Generate a random word 1000 times using this context. How many times is the word \"husband\" generated? What is the empirical probability of generation based on your output? Does this match the output of `word_prob(\"by her\",\"husband\")`? _(5 points)_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVkNgjms2K6P"
      },
      "source": [
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qif-zuVy7kE4"
      },
      "source": [
        "__Answer:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy7Rtc7L6psV"
      },
      "source": [
        "__Question:__ Generate a random text of length 100 words. Comment on the local and global semantics of the generated text. Now train a 4-gram LM on the same data and generate a 100-word text again. Do you observe any differences between the outputs of the two models? _(5 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK5dIpby7ObT"
      },
      "source": [
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-oFghvN7rss"
      },
      "source": [
        "__Answer:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxh0pOSn9N1O"
      },
      "source": [
        "### Evaluating the LM: Perplexity\n",
        "\n",
        "In the context of language modeling, perplexity measures how an LM predicts a sample. It is computed as the per word inverse probability of a held-out set:\n",
        "\n",
        "$$ Perplexity(W) = P(W_1 W_2 \\ldots W_N)^{-1/N} $$\n",
        "\n",
        "Complete the following function which computes the perplexity of an ngram language model given the class object and a dataset (represented as a list of strings as done earlier). _(10 points)_\n",
        "\n",
        "__Note 1:__ You may assume that the text is normalized as done before, so no text processing is required in the function.\n",
        "\n",
        "__Note 2:__ Consider performing computations in the log domain to avoid underflow errors. Recall the log equalities:\n",
        "\n",
        "$$ P = 2^{\\log_2 P} $$\n",
        "$$ \\log (a_1 a_2 \\ldots a_N)^{1/N} = \\frac{1}{N}\\left( \\log a_1 + \\log a_2 + \\ldots + \\log a_N \\right) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg5Ezw1PK4fo"
      },
      "source": [
        "import math\n",
        "\n",
        "def perplexity(obj, data):\n",
        "  \"\"\"Function to compute perplexity of ngram LM.\n",
        "\n",
        "  Parameters:\n",
        "    obj (class object): NgramLM class object\n",
        "    text (list(str)): list of sentences\n",
        "\n",
        "  Returns:\n",
        "    perp (float): perplexity of the LM on given string\n",
        "  \"\"\"\n",
        "  # TODO: Your code here\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drgo2ZhMgvTA"
      },
      "source": [
        "__Question:__ What is the perplexity of the model on the training corpus? _(1 point)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWiy10LSNhl2"
      },
      "source": [
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6kIPjp58mDa"
      },
      "source": [
        "__Question:__ What is the perplexity of the 4-gram LM you trained earlier on the training corpus? _(1 point)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRkn-YB-8r_P"
      },
      "source": [
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giDoTrs9ORpD"
      },
      "source": [
        "You will now use your above implementation to evaluate your model on a small held out development set from Leo Tolstoy's Anna Karenina. First we download and preprocess this data similar to how we did for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WTvnRSF5nhD"
      },
      "source": [
        "# Download Anna Karenina\n",
        "! wget http://www.gutenberg.org/files/1399/1399-0.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwmOEmrt5sMW"
      },
      "source": [
        "# Process the text file to get the contents of Chapter 1\n",
        "with open('1399-0.txt', 'r') as file:\n",
        "    dev_raw = file.read().replace('\\n', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWqwXVzo6UJj"
      },
      "source": [
        "import re\n",
        "\n",
        "pattern = \"Chapter 1(.*)Chapter 2\"\n",
        "dev_ch1 = re.search(pattern, dev_raw).group(1)\n",
        "\n",
        "sentences = sent_tokenize(dev_ch1)\n",
        "\n",
        "dev_text = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for sentence in sentences:\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  dev_text.append(\" \".join([token.lower() for token in tokens]))\n",
        "\n",
        "print (\"Dev data has {} sentences\".format(len(dev_text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi8ach2U7Tx3"
      },
      "source": [
        "__Question:__ Compute the perplexity of the 3-gram LM on the development set prepared above. What is the reason for this perplexity value? _(3 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-qiPw6p7PcG"
      },
      "source": [
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C1nSb2o7u_B"
      },
      "source": [
        "__Answer:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJRKm-HE8B7Q"
      },
      "source": [
        "### Zeros and generalization\n",
        "\n",
        "From the above, you would have realized that our trigram LM in the barebones setting is probably not robust enough to be deployed in general settings, due to the data sparsity problem. This problem is dealt with by using \"smoothing\" methods for unseen n-grams and the `<UNK>` token for OOV words.\n",
        "\n",
        "In this section, you will implement Laplace (add-one) smoothing and use the `<UNK>` token for handling OOV words in the evaluation set. Complete the following class definition to achieve this. _(15 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6a4wXKvTf6E"
      },
      "source": [
        "class NgramLMWithLaplaceSmoothing(object):\n",
        "  \"\"\"An n-gram language model with OOV handling and Laplace smoothing.\"\"\"\n",
        "\n",
        "  # TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE7UsXK5UL-_"
      },
      "source": [
        "__Question:__ Report the perplexity of the new LM on the development data. (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0aXL2hUPFQ4"
      },
      "source": [
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kWzYeO5UZ1D"
      },
      "source": [
        "__Question:__ Can you think of a different way to solve the OOV problem? _(2 points)_\n",
        "\n",
        "__Answer:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Kjgo2NSc1A"
      },
      "source": [
        "__Question (extra credit):__ Laplace smoothing is a relatively naive smoothing method. In the lectures, you learnt about more advanced methods: Good-Turing, Backoff, Interpolation, Kneser-Ney. Implement any one of these smoothing methods (pick your favorite). Evaluate the resulting trigram LM on the development data and report the perplexity. Is it better than the simple Laplace smoothing? _(10 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HapmEPF4cDUa"
      },
      "source": [
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngAvmJ3qPNab"
      },
      "source": [
        "__Answer:__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwOF-lAWVE94"
      },
      "source": [
        "## Part 2: Parsing and the CYK algorithm\n",
        "\n",
        "In the lecture on Syntax, you learnt about parsing algorithms, including the bottom-up CYK algorithm. In this section, you will implement the CYK algorithm for computing the parse tree of a sentence given a grammar.\n",
        "\n",
        "You may look at the pseudocode on [Wikipedia](https://en.wikipedia.org/wiki/CYK_algorithm#As_pseudocode) or refer to descriptions of the CYK algorithm online (such as [this](https://courses.engr.illinois.edu/cs373/sp2009/lectures/lect_15.pdf)), but you may not copy code directly from another source. The objective of this exercise is to familiarize yourself with parsing.\n",
        "\n",
        "First, we will provide some starter code to load a simple grammar which can be used to test your implementation. The CYK algorithm only works with context-free grammars (CFGs) in the [Chomsky Normal Form (CNF)](https://en.wikipedia.org/wiki/Chomsky_normal_form), but any CFG can be represented as an equivalent CNF. You can use NLTK to check if the grammar is in CNF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44xCk4nMYVWL"
      },
      "source": [
        "# grammar rules\n",
        "\n",
        "cfg_rules=\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N\n",
        "NP -> Det N PP\n",
        "NP -> 'I'\n",
        "VP -> V NP\n",
        "VP -> VP PP\n",
        "Det -> 'an'\n",
        "Det -> 'my'\n",
        "N -> 'elephant'\n",
        "N -> 'pajamas'\n",
        "V -> 'shot'\n",
        "P -> 'in'\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npLfx9CveUeI"
      },
      "source": [
        "__Question:__ Use NLTK to check if the grammar `cfg` is in the Chomsky Normal Form. _(1 point)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVY6CyYhc1zn"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uatLXceVepso"
      },
      "source": [
        "__Question:__ Convert the above CFG into CNF (use pen and paper) and create a new grammar using it. Use NLTK to verify if it is in CNF. _(4 points)_\n",
        "\n",
        "Here are the steps to convert any CFG into a CNF:\n",
        "\n",
        "1. Eliminate start symbol from the RHS. If the start symbol S is at the right-hand side of any production, create a new production as: S1 -> S\n",
        "2. If CFG contains null, unit or useless production rules, eliminate them.\n",
        "3. Eliminate terminals from RHS if they exist with other terminals or non-terminals.\n",
        "4. Eliminate RHS with more than two non-terminals.\n",
        "\n",
        "(Hint: There is only one offending rule in the above grammar.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgnhok80g5io"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Write the CNF grammar here as a string\n",
        "\n",
        "# TODO: Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkL26o73dKJq"
      },
      "source": [
        "You can now use the above grammar and the sentence: _\"I shot an elephant in my pajamas\"_ to demonstrate your implementation of the CYK parser.\n",
        "\n",
        "Complete the following code block to implement the parser. We have provided the definition of the Node class which stores a non-terminal, and some boilerplate code to ease you into the implementation. Your main task is to implement the `parse()` function, which generates the parse table in a bottom-up manner. The `parse_table` in the `CYKParser` class below can be thought of as a table which contains number of rows equal to the number of words in the sentence. _(25 points)_\n",
        "\n",
        "_Hint: It may be beneficial to first run through the algorithm for the given grammar and the sentence on pen and paper._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk2mYJWnhomu"
      },
      "source": [
        "class Node:\n",
        "  \"\"\"\n",
        "  Equivalent to a non-terminal. Since our grammar is CNF, a node can have at\n",
        "  most 2 children. Following 2 cases are possible:\n",
        "  Case 1 -> child1 is a terminal symbol\n",
        "  Case 2 -> both child1 and child2 are Nodes.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, symbol, child1, child2=None):\n",
        "    self.symbol = symbol\n",
        "    self.child1 = child1\n",
        "    self.child2 = child2\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"\n",
        "    :return: the string representation of a Node object.\n",
        "    \"\"\"\n",
        "    return self.symbol\n",
        "\n",
        "class CYKParser:\n",
        "  \"\"\"\n",
        "  A CYK parser which is able to parse any grammar in CNF. The parser object\n",
        "  is created from a CNF grammar and a corresponding sentence.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, grammar, sentence):\n",
        "    \"\"\"\n",
        "    Creates a new parser object.\n",
        "\n",
        "    Parameters:\n",
        "      grammar (string): input grammar as string of rules\n",
        "      sentence (string): sentence to be parsed\n",
        "    \"\"\"\n",
        "    self.parse_table = None\n",
        "    self.prods = {}\n",
        "    self.grammar = self.read_grammar(grammar)\n",
        "    self.input = sentence.split()\n",
        "\n",
        "  def read_grammar(self, grammar):\n",
        "    \"\"\"\n",
        "    Reads the grammar as a string and stores it in the form\n",
        "    of a list.\n",
        "    \"\"\"\n",
        "    return [x.replace(\"->\", \"\").split() for x in grammar.strip().split(\"\\n\")]\n",
        "\n",
        "  def parse(self):\n",
        "    \"\"\"\n",
        "    Does the actual parsing according to the CYK algorithm.\n",
        "    Stores the parse table in self.parse_table\n",
        "    \"\"\"\n",
        "    length = len(self.input)\n",
        "    # self.parse_table[y][x] is the list of nodes in the x+1 cell \n",
        "    # of y+1 row in the table. That cell covers the word below it \n",
        "    # and y more words after.\n",
        "    self.parse_table = [[[] for x in range(length - y)] \n",
        "                        for y in range(length)]\n",
        "\n",
        "\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "  def print_tree(self):\n",
        "    \"\"\"\n",
        "    Print the parse tree starting with the start symbol.\n",
        "    \"\"\"\n",
        "    start_symbol = self.grammar[0][0]\n",
        "    final_nodes = [n for n in self.parse_table[-1][0] if n.symbol == start_symbol]\n",
        "    if final_nodes:\n",
        "      print(\"\\nPossible parse(s):\")\n",
        "      trees = [self.generate_tree(node) for node in final_nodes]\n",
        "      for tree in trees:\n",
        "        print(tree)\n",
        "    else:\n",
        "      print(\"The given sentence is not contained in the language produced by the given grammar!\")\n",
        "\n",
        "  def generate_tree(self, node):\n",
        "    \"\"\"\n",
        "    Generates the string representation of the parse tree.\n",
        "    :param node: the root node.\n",
        "    :return: the parse tree in string form.\n",
        "    \"\"\"\n",
        "    if node.child2 is None:\n",
        "        return f\"[{node.symbol} '{node.child1}']\"\n",
        "    return f\"[{node.symbol} {self.generate_tree(node.child1)} {self.generate_tree(node.child2)}]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbDKhmkgCGmu"
      },
      "source": [
        "parser = CYKParser(cnf_rules, \"I shot an elephant in my pajamas\")\n",
        "parser.parse()\n",
        "parser.print_tree()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}